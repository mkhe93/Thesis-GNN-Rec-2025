field_separator: "\t"
USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
TIME_FIELD: timestamp
user_inter_num_interval: "[20,inf)"
load_col:
    inter: [user_id, item_id, timestamp]
normalize_all: false

# Evaluation
eval_args:                            # (dict) 4 keys: group_by, order, split, and mode
  split: {'LK': ['valid_and_test', 5]}   # (dict) The splitting strategy ranging in ['RS','LS'].{'RS':[0.8,0.1,0.1]}, valid_and_test, test_only, valid_only
  #split: { 'RS':[0.7,0.1,0.2] }       # (dict) The splitting strategy ranging in ['RS','LS'].{'RS':[0.8,0.1,0.1]}
  group_by: user                      # (str) The grouping strategy ranging in ['user', 'none'].
  order: TO                           # (str) The ordering strategy ranging in ['RO', 'TO'].
  mode: full

# Training
# permanent
shuffle: False
eval_batch_size: 256           # (int) The training batch size.
train_batch_size: 128          # (int) The training batch size.
learner: adam                   # (str) The name of used optimizer.
show_progress: False
valid_metric: "ndcg@10"
save_step: 5
eval_step: 5
stopping_step: 10
topUser: 10

train_neg_sample_args:          # (dict) Negative sampling configuration for model training.
  distribution: uniform         # (str) The distribution of negative items.
  sample_num: 1                 # (int) The sampled num of negative items.
  alpha: 1.0                    # (float) The power of sampling probability for popularity distribution.
  dynamic: False                # (bool) Whether to use dynamic negative sampling.
  candidate_num: 1              # (int) The number of candidate negative items when dynamic negative sampling.

# best hyper parameters
epochs: 150
embedding_size: 64
learning_rate: 0.001
lambda: 0.05
gamma: 0.0001
ii_neighbor_num: 10
initial_weight: 0.0001
negative_num: 500
negative_weight: 200
w1: 0.000001
w2: 1.0
w3: 0.00001
w4: 1.0